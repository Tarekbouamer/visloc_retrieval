{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook helps to choose best model to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../asmk\")\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import argparse\n",
    "from random import sample\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Timm\n",
    "import timm \n",
    "\n",
    "# Core\n",
    "from core.utils.configurations              import make_config, config_to_string\n",
    "\n",
    "# Image Retrieval\n",
    "from image_retrieval.utils.io               import create_experiment_file_from_cfg , create_experiment_file\n",
    "from image_retrieval.configuration          import DEFAULTS as DEFAULT_CONFIGS\n",
    "from image_retrieval.tools                  import make_dataloader, make_model, compute_PCA_layer, get_data_sample\n",
    "from image_retrieval.tools                  import train, validate, test\n",
    "from image_retrieval.modules.heads.head     import globalHead\n",
    "from image_retrieval.models.GF_net          import ImageRetrievalNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "class Parameters:\n",
    "    config_path     = \"./image_retrieval/configuration/defaults/search_timm_models.ini\"\n",
    "    directory       = \"./experiments/\"\n",
    "    data            = \"/media/dl/Data/datasets/\"\n",
    "    models_family   = \"resnet\"\n",
    "\n",
    "params = Parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multi-processing\n",
    "rank, world_size = 0, 1\n",
    "device = torch.device(0)\n",
    "\n",
    "# Set device\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "    \n",
    "# Load configuration\n",
    "cfg = make_config(config_path=params.config_path, \n",
    "                  defauls=DEFAULT_CONFIGS[\"empty\"],\n",
    "                  logger=logger)\n",
    "\n",
    "# Experiment Path, from cfg\n",
    "exp_dir = create_experiment_file(params.directory, extension=\"search_timm_model\", logger=logger)\n",
    "\n",
    "# Initialize logging\n",
    "logger.info(\"\\n%s\", config_to_string(cfg))\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_dl, _ = make_dataloader(params, cfg, rank, world_size,\n",
    "                                          logger=logger)\n",
    "    \n",
    "# Sample data \n",
    "sample_dl =  get_data_sample(cfg, train_dl, logger=logger)\n",
    "\n",
    "\n",
    "# Get timm models list\n",
    "timm_model_list = timm.list_models(pretrained=True, filter=params.models_family+\"*\")\n",
    "logger.info(\"%s %s model found \", len(timm_model_list), params.models_family)\n",
    "    \n",
    "# CSV file \n",
    "fileds = ['Model', 'Dim', 'E', 'M', 'H',]\n",
    "    \n",
    "with open(\"search_timm_models_\" + params.models_family +\".csv\", 'w') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fileds)\n",
    "    writer.writeheader()\n",
    "        \n",
    "    for model_name in timm_model_list:\n",
    "            \n",
    "        # parse params with default values\n",
    "        body_cfg     = cfg[\"body\"]\n",
    "        global_cfg   = cfg[\"global\"]\n",
    "            \n",
    "        body_cfg[\"name\"]              = model_name\n",
    "        body_cfg[\"features_scales\"]   = str([1, 2, 3, 4])\n",
    "\n",
    "            \n",
    "        # Create backbone            \n",
    "        logger.info(\"Creating backbone model: %s  with features_scales: %s\",  model_name,\n",
    "                                                                                str(body_cfg[\"features_scales\"]))\n",
    "            \n",
    "        # Load model state dictionary\n",
    "        body = timm.create_model(model_name, \n",
    "                                    features_only=True, \n",
    "                                    out_indices=body_cfg.getstruct(\"features_scales\"), \n",
    "                                    pretrained=True,\n",
    "                                    )\n",
    "            \n",
    "        body_channels           = body.feature_info.channels()\n",
    "        body_reductions         = body.feature_info.reduction()\n",
    "        body_module_names       = body.feature_info.module_name()\n",
    "            \n",
    "        logger.debug(\"Body channels: %s    Reductions: %s      Layer_names: %s\",   body_channels, \n",
    "                                                                                    body_reductions,\n",
    "                                                                                    body_module_names)\n",
    "            \n",
    "        # model redunction\n",
    "        body_dim = body_channels[-1]\n",
    "        if global_cfg.getboolean(\"reduction\"):\n",
    "            out_dim = global_cfg["global"]_dim\n",
    "        else:\n",
    "            out_dim =  body_dim \n",
    "            \n",
    "        # Head\n",
    "        global_head = globalHead(   inp_dim=body_dim,\n",
    "                                        global_dim=out_dim,\n",
    "                                        local_dim=128,\n",
    "                                        \n",
    "                                        pooling=global_cfg.getstruct(\"pooling\"),\n",
    "                                        do_withening=global_cfg.getboolean(\"whithening\"),\n",
    "                                        layer=global_cfg.get(\"type\"),\n",
    "                                        )\n",
    "        # freeze head \n",
    "        if global_cfg.getboolean(\"freeze_whiten\"):\n",
    "            logger.debug(\"Freeze withening  paramters\")\n",
    "            for p in global_head.whiten.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "            for p in global_head.local_whiten.parameters():\n",
    "                p.requires_grad = False\n",
    "            \n",
    "        # Create a generic image retrieval network\n",
    "        model = ImageRetrievalNet(body, global_head)  \n",
    "            \n",
    "            \n",
    "        # Compute PCA\n",
    "        compute_PCA_layer(model, sample_dl, params, cfg,\n",
    "                              device=device, \n",
    "                              logger=logger)\n",
    "                \n",
    "            # # Load model, and init PCA layer\n",
    "            # model, out_dim = make_model(args, config, train_dataloader, \n",
    "            #                                 rank=rank, \n",
    "            #                                 world_size=world_size, \n",
    "            #                                 device=device,\n",
    "            #                                 log_debug=log_debug,\n",
    "            #                                 log_info=log_info)\n",
    "\n",
    "        # Init GPU stuff\n",
    "        model = model.cuda(device)\n",
    "            \n",
    "        # Eval\n",
    "        logger.info(\"Evaluate model { %s }\" , model_name)\n",
    "\n",
    "        scores , _ = test(params, cfg, model, \n",
    "                            train_imgs=train_dl.dataset.images,\n",
    "                            rank=rank, \n",
    "                            world_size=world_size,\n",
    "                            out_dim=out_dim,\n",
    "                            device=device,\n",
    "                            logger=logger)\n",
    "\n",
    "\n",
    "        scores[\"Model\"] = model_name\n",
    "        scores[\"Dim\"]   = out_dim\n",
    "            \n",
    "        # Write to csv\n",
    "        writer.writerow(scores)\n",
    "    \n",
    "logger.info(\"Evaluation Done ..... \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('loc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e22d42cf0e624264a2f037cfc13571c20a9e1caed804176fe8ec1fe565d6bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
