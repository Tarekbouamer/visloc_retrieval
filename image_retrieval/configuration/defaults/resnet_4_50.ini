[general]
gpu_id = 0
log_interval = 20
val_interval = 5
test_interval = 5

cudnn_benchmark=False

[body]
arch = resnet_4_50

# pre_trained
pretrained = True
source_url = pytorch

# init body

initializer = xavier_normal
                                # supported: xavier_[normal,uniform], kaiming_[normal,uniform], orthogonal

weight_gain_multiplier = 1
                                # note: this is ignored if weight_init == kaiming_
                                #    "weight_gain_multiplier": 1,  # note: this is ignored if weight_init == kaiming_*
                                #    "weight_init": "xavier_normal"  # supported: xavier_[normal,uniform], kaiming_[normal,uniform], orthogonal

activation = relu
                                #   Activation: 'leaky_relu' or 'elu'
; activation_slope = 0.01

# Additional parameters for the body
body_params = {"classes":0}

# Number of frozen modules: in [1, 5]
num_frozen = 0

# Normalization frozen 
bn_frozen = no

out_channels = {"mod1": 64, "mod2": 256, "mod3": 512, "mod4": 1024, "mod5": 2048}
#out_strides = {"mod1": 4, "mod2": 4, "mod3": 8, "mod4": 16, "mod5": 32}

# Normalization mode:
# -- bn: in-place batch norm everywhere
# -- syncbn: synchronized in-place batch norm everywhere
# -- syncbn+bn: synchronized in-place batch norm in the static part of the network, in-place batch norm everywhere else
# -- gn: group norm everywhere
# -- syncbn+gn: synchronized in-place batch norm in the static part of the network, group norm everywhere else
# -- off: do not normalize activations (scale and bias are kept)
normalization_mode = bn

# Group norm parameters
gn_groups = 16

[global]
# Loss settings
loss = triplet
loss_margin = 0.5

# Modules settings
pooling = {"name": "GeM", "params": {"p":3, "eps": 1e-6}}

# whithening 
type=linear
whithening = True

# dim
inp_dim     = 1024
global_dim  = 1024
local_dim   = 128

# multi - scales
scales = [1.414, 1.0, 0.707]

#PCA
num_samples = 500000
update = False
freeze = True

[local]
num_features= 500
# multi - scales
scales = [2.0, 1.414, 1.0, 0.707, 0.5, 0.353, 0.25]

[optimizer]
#type
type = AdamW

#lr
lr = 1e-6
lr_coefs = {"body": 1, "pool": 10, "whiten": 1}

# weight_decay
weight_decay = 1e-4
weight_decay_coefs = {"body": 1, "pool": 0, "whiten": 0}
weight_decay_norm = yes

#clipping
grad_mode = norm  
grad_value = 1.0
grad_type = 1.0

# additional
momentum = 0.9
nesterov = yes

# retrieval, features
loss_weights = (1.,)

[scheduler]
epochs = 100
# Scheduler type: 'linear', 'step', 'exp', 'poly' or 'multistep'
type = exp
# When to update the learning rate: 'batch', 'epoch'
update_mode = epoch
# Additional parameters for the scheduler
# -- linear
#   from: initial lr multiplier
#   to: final lr multiplier
# -- step
#   step_size: number of steps between lr decreases
#   gamma: multiplicative factor
# -- poly
#   gamma: exponent of the polynomial
# -- multistep
#   milestones: step indicies where the lr decreases will be triggered
params = {"gamma":0.99}

burn_in_steps = 3
burn_in_start = 0.25

[dataloader]
dataset = retrieval-SfM-120k
neg_num = 5

query_size = 2000
pool_size  = 20000

min_size = 200
max_size = 1024

batch_size = 5
num_workers = 14

[augmentaion]

use_prefetcher = False
no_aug = False
scale = (0.8, 1.2)
ratio = (0.75, 1.33)
hflip = 0.4
vflip = 0.4
color_jitter = 0.4
auto_augment = rand-m5-n3-mstd0.2
interpolation = bilinear
mean = (0.485, 0.456, 0.406)
std = (0.229, 0.224, 0.225)
re_prob = 0.25
re_mode = pixel
re_count = 1
re_num_splits = 0
crop_pct = None
tf_preprocessing = False
separate = True

[test]
; test_datasets = ["roxford5k"]

datasets = ["val_eccv20", "roxford5k", "rparis6k"]
batch_size = 1
min_size = 200
max_size = 1024
num_workers = 14

mode = global_descriptor
multi_scale = False