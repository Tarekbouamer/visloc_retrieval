[general]
gpu_id = 0
log_interval = 10
val_interval = 5
test_interval = 5

cudnn_benchmark=False

[body]
arch = resneck50

# pre_trained
pretrained = True
source_url = pytorch
path = classification/experiments/checkpoint.pth

# init body
initializer = xavier_normal
                                # supported: xavier_[normal,uniform], kaiming_[normal,uniform], orthogonal

weight_gain_multiplier = 1
                                # note: this is ignored if weight_init == kaiming_
                                #    "weight_gain_multiplier": 1,  # note: this is ignored if weight_init == kaiming_*
                                #    "weight_init": "xavier_normal"  # supported: xavier_[normal,uniform], kaiming_[normal,uniform], orthogonal

activation = relu
                                #   Activation: 'leaky_relu' or 'elu'
; activation_slope = 0.01

# Additional parameters for the body
body_params = {"classes":0, "drop_path_rate": 0,}

# Number of frozen modules: in [1, 5]
num_frozen = 0

# Normalization frozen 
bn_frozen = no

out_channels = {"mod1": 64, "mod2": 256, "mod3": 512, "mod4": 1024, "mod5": 2048}
#out_strides = {"mod1": 4, "mod2": 4, "mod3": 8, "mod4": 16, "mod5": 32}

# Normalization mode:
# -- bn: in-place batch norm everywhere
# -- syncbn: synchronized in-place batch norm everywhere
# -- syncbn+bn: synchronized in-place batch norm in the static part of the network, in-place batch norm everywhere else
# -- gn: group norm everywhere
# -- syncbn+gn: synchronized in-place batch norm in the static part of the network, group norm everywhere else
# -- off: do not normalize activations (scale and bias are kept)
normalization_mode = br+bg

# Group norm parameters
gn_groups = 16

[global]
# Loss settings
loss = triplet
loss_margin = 0.5

# Modules settings
pooling = {"name": "GeM", "params": {"p":3, "eps": 1e-6}}

# whithening 
whithening = True
out_dim = 512

# PCA
num_samples = 10000000
update = True


[optimizer]
type = Adam

lr = 5e-7
lr_coefs = {"body": 1, "encoders": 1, "pool": 10, "whiten": 1}

weight_decay = 1e-6
weight_decay_coefs = {"body": 1, "encoders": 1, "pool": 0, "whiten": 10}

weight_decay_norm = yes

momentum = 0.9
nesterov = yes

# retrieval, features
loss_weights = (1.,)


[scheduler]
epochs = 200
# Scheduler type: 'linear', 'step', 'exp', 'poly' or 'multistep'
type = exp
# When to update the learning rate: 'batch', 'epoch'
update_mode = epoch
# Additional parameters for the scheduler
# -- linear
#   from: initial lr multiplier
#   to: final lr multiplier
# -- step
#   step_size: number of steps between lr decreases
#   gamma: multiplicative factor
# -- poly
#   gamma: exponent of the polynomial
# -- multistep
#   milestones: step indicies where the lr decreases will be triggered
params = {"gamma":0.99}

burn_in_steps = 2
burn_in_start = 0.25


[dataloader]
training_dataset = retrieval-SfM-120k
neg_num = 5

train_query_size = 2000
train_pool_size  = 20000

train_shortest_size = 200
train_longest_max_size = 1024

test_shortest_size = 200
test_longest_max_size = 1024

rgb_mean = (0.485, 0.456, 0.406)
rgb_std = (0.229, 0.224, 0.225)

random_flip = yes
random_scale = None

train_batch_size = 5
test_batch_size = 1

update_every = 1
num_workers = 6

test_datasets = ["roxford5k", "rparis6k"]
